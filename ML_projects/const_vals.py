# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/CONSTANTS.ipynb.

# %% auto 0
__all__ = ['RANDOM_STATE', 'N_JOBS', 'VERBOSE', 'N_ITERATIONS_RFR', 'CV_RFR', 'N_CALLS', 'RANDOM_GRID_RFR', 'RANDOM_GRID_XGB',
           'RANDOM_GRID_SVR', 'RANDOM_RIDGE_REGRESSION', 'RANDOM_KNEIGHBORSR_REGRESSION',
           'RANDOM_GRADIENT_BOOST_REGRESSION', 'RANDOM_ADA_BOOST_REGRESSION', 'algorithm_to_params',
           'algorithm_to_model']

# %% ../nbs/CONSTANTS.ipynb 3
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.svm import SVR
from skopt.space import Real, Categorical, Integer
from skopt.utils import use_named_args
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor

# %% ../nbs/CONSTANTS.ipynb 4
RANDOM_STATE = 42 
N_JOBS = -1 
VERBOSE = 1

# %% ../nbs/CONSTANTS.ipynb 5
N_ITERATIONS_RFR = 100
CV_RFR = 3
N_CALLS = 20


# %% ../nbs/CONSTANTS.ipynb 6
RANDOM_GRID_RFR={'bootstrap': [True, False],
                'max_depth': [3,10, 25, 45, 70],
                'max_features': ['sqrt', 'log2'],
                'min_samples_leaf': [1, 2, 4],
                'min_samples_split': [2, 5, 10],
                'n_estimators': [200, 400, 600, 800]}

# SPACE_RFR = {'bootstrap' : Categorical([True, False]),
#              'max_depth' : Integer(1,100),
#              'max_features' : Categorical(['sqrt', 'log2']),
#              'min_samples_leaf': Integer(1,5),
#              'min_samples_split' : Integer(1,10),
#              'n_estimators' : Integer(100,800)}


RANDOM_GRID_XGB = {
    "n_estimators": [10, 50, 100],
    "subsample":[0.6, 0.8, 1],
    "learning_rate":[0.01, 0.1, 0.5, 1],
    "gamma":[0.01, 0.1, 1, 5],
    "colsample_bytree":[0.5, 0.7, 0.9, 1],
    "alpha":[0, 0.1, 0.5]
}


RANDOM_GRID_SVR = {
    "kernel" : ["linear","sigmoid","poly"],
    "degree" : [2,3,5,7],
    "gamma" : ["scale","auto"],
    "epsilon" : [0.1 , 0.5 , 0.9]
    }

RANDOM_RIDGE_REGRESSION = {
  "alpha" : [1,3,5],
  "solver" : ["auto","svd","cholesky","sag"]
  }

RANDOM_KNEIGHBORSR_REGRESSION = {
  "n_neighbors" : [3,5,7,9],
  "weights" : ["uniform","distance"],
  "algorithm" : ["auto"],
  }

RANDOM_GRADIENT_BOOST_REGRESSION = {
  "loss" : ["squared_error", "absolute_error", "huber", "quantile"],
  "learning_rate":[0.01, 0.1, 0.5, 1],
  "n_estimators": [200, 400, 600, 800],
  'min_samples_split': [2, 5, 10],
  'max_depth': [3,10, 25, 45, 70],
  'max_features': ['sqrt', 'log2'],
  }

RANDOM_ADA_BOOST_REGRESSION={
  "n_estimators": [200, 400, 600, 800],
  "learning_rate":[0.01, 0.1, 0.5, 1],
  "loss" : ["linear", "square", "exponential"]
  }






# %% ../nbs/CONSTANTS.ipynb 7
algorithm_to_params = {
  'RFR' : RANDOM_GRID_RFR,
  'XGB' : RANDOM_GRID_XGB,
  'SVR' : RANDOM_GRID_SVR,
  'RIDGE' : RANDOM_RIDGE_REGRESSION,
  'KNEIGHBORS' : RANDOM_KNEIGHBORSR_REGRESSION,
  'GRADIENT_BOOST' : RANDOM_GRADIENT_BOOST_REGRESSION,
  'ADA' : RANDOM_ADA_BOOST_REGRESSION
  }


algorithm_to_model ={
  'RFR' : RandomForestRegressor() ,
  'XGB' : xgb.XGBRegressor(),
  'SVR' : SVR(),
  'RIDGE' : Ridge(),
  'KNEIGHBORS' : KNeighborsRegressor(),
  'GRADIENT_BOOST' : GradientBoostingRegressor() ,
  'ADA' : AdaBoostRegressor()
}
