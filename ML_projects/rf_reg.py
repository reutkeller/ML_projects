# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/random_forest_reg.ipynb.

# %% auto 0
__all__ = ['TrainRFReg']

# %% ../nbs/random_forest_reg.ipynb 3
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from skopt import BayesSearchCV,gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error

from . import const_vals as CONST


# %% ../nbs/random_forest_reg.ipynb 4
class TrainRFReg():
  def __init__(self,
      train_test_data : list , # list with train/test data , in this order : [x_train,x_test,y_train,y_test]
      hyper_method : str , #hyperparameter tunning method. accepts : 'randomized' 'bayesian' , 'bayesian continous'
      hyper_params : dict = CONST.RANDOM_GRID_RFR , #parameters for hyperparameter tunning
      space : list = CONST.SPACE_RFR , #space for continues bayesian continous search 
      ):
     self.hyper_method = hyper_method
     self.hyper_params = hyper_params

     self.x_train = train_test_data[0]
     self.x_test = train_test_data[1]
     self.y_train = train_test_data[2]
     self.y_test = train_test_data[3]
     
     # space for bayesian continous searching
     self.space = space

     #generate random forest regressor 
     self.model = RandomForestRegressor()

     # train best model and get y_pred
     self.best_model , self.y_pred = self.hyperparameter()

     self.evaluate_model()

  def hyperparameter(self):
     
     if self.hyper_method == 'randomized':
       
       rf_random = RandomizedSearchCV(estimator = self.model, 
                                      param_distributions = self.hyper_params,
                                      n_iter = CONST.N_ITERATIONS_RFR,
                                      cv = CONST.CV_RFR, 
                                      verbose=CONST.VERBOSE , 
                                      random_state=CONST.RANDOM_STATE , 
                                      n_jobs = CONST.N_JOBS)
       
       #fit model 
       rf_random.fit(self.x_train, self.y_train)

       #best params
       self.best_params = rf_random.best_params_


     elif self.hyper_method == 'bayesian':
      
      rf_bayes = BayesSearchCV(self.model,
                      search_spaces=self.hyper_params, 
                      n_iter=CONST.N_ITERATIONS_RFR, 
                      cv=CONST.CV_RFR)
      np.int = int
      
       #fit model 
      rf_bayes.fit(self.x_train, self.y_train)

      self.best_params = rf_bayes.best_params_

      print(f'best params : {rf_bayes.best_params_}')

      
      
     #train best model
     self.best_model = RandomForestRegressor(**self.best_params)

     # fit best model
     self.best_model.fit(self.x_train, self.y_train)

     #predict test data
     self.y_pred = self.best_model.predict(self.x_test)

     return self.best_model , self.y_pred
  

  def evaluate_model(self):

    # Calculate evaluation metrics
    mae = mean_absolute_error(self.y_test, self.y_pred)
    mse = mean_squared_error(self.y_test, self.y_pred)
    rmse = np.sqrt(mse)

    
    # Mean Absolute Percentage Error (MAPE)
    mape = np.mean(np.abs((self.y_test - self.y_pred) / self.y_test)) * 100
    print(f'MAE : {mae} , MSE: {mse} , RMSE : {rmse} MAPE : {mape} ')

    # Plot y_true vs y_test
    plt.scatter(self.y_test, self.y_pred)
    plt.xlabel('True Values')
    plt.ylabel('Predictions')
    plt.title('True Values vs Predictions')
    plt.show()

  
